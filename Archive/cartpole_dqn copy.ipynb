{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54c8dfaa-b6d5-44ce-b31a-e48a06347f9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T02:54:36.122087Z",
     "iopub.status.busy": "2023-11-20T02:54:36.121687Z",
     "iopub.status.idle": "2023-11-20T02:54:46.801026Z",
     "shell.execute_reply": "2023-11-20T02:54:46.800065Z",
     "shell.execute_reply.started": "2023-11-20T02:54:36.122048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /usr/local/lib/python3.9/dist-packages (0.29.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (1.23.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (2.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (6.0.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gymnasium) (3.11.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: gymnasium[accept-rom-license] in /usr/local/lib/python3.9/dist-packages (0.29.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium[accept-rom-license]) (4.4.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from gymnasium[accept-rom-license]) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium[accept-rom-license]) (6.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium[accept-rom-license]) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium[accept-rom-license]) (1.23.4)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.9/dist-packages (from gymnasium[accept-rom-license]) (0.4.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (8.1.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.28.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (4.64.1)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.9/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (0.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gymnasium[accept-rom-license]) (3.11.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license]) (2.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: opencv-python in /usr/local/lib/python3.9/dist-packages (4.6.0.66)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from opencv-python) (1.23.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install gymnasium[accept-rom-license]\n",
    "# !apt-get update\n",
    "# !apt-get install ffmpeg libsm6 libxext6  -y\n",
    "# !apt install -y libgl1-mesa-glx\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "769d1335-d30e-4339-8d64-d35076b04f17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T02:54:46.803378Z",
     "iopub.status.busy": "2023-11-20T02:54:46.803091Z",
     "iopub.status.idle": "2023-11-20T02:54:50.207324Z",
     "shell.execute_reply": "2023-11-20T02:54:50.206339Z",
     "shell.execute_reply.started": "2023-11-20T02:54:46.803350Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#adapted from https://www.kaggle.com/code/danieldreher/vanilla-dqn-cartpole-tensorflow-2-3/notebook \n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rand\n",
    "import cv2\n",
    "import collections\n",
    "from collections import deque\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a12ad070-b7e5-4d63-ade0-b82da1e951af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T02:54:50.208896Z",
     "iopub.status.busy": "2023-11-20T02:54:50.208405Z",
     "iopub.status.idle": "2023-11-20T02:54:50.220901Z",
     "shell.execute_reply": "2023-11-20T02:54:50.219796Z",
     "shell.execute_reply.started": "2023-11-20T02:54:50.208869Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Downsamples image to 84x84\n",
    "    Greyscales image\n",
    "\n",
    "    Returns numpy array\n",
    "    \"\"\"\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\"\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    \"\"\"Normalize pixel values in frame --> 0 to 1\"\"\"\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "    \n",
    "def make_env(env, obs_type=\"grayscale\"):\n",
    "    env = ProcessFrame84(env)\n",
    "    return ScaledFloatFrame(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef65d081-8715-47cf-972d-873003fa529d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T02:54:50.222675Z",
     "iopub.status.busy": "2023-11-20T02:54:50.222248Z",
     "iopub.status.idle": "2023-11-20T02:54:50.231594Z",
     "shell.execute_reply": "2023-11-20T02:54:50.230320Z",
     "shell.execute_reply.started": "2023-11-20T02:54:50.222602Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#the replay buffer contains episode transitions in the\n",
    "#  order the episode is generated\n",
    "#The Python collections deque has a pointer to the next\n",
    "#  and previous element for all elements\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.replay_memory = deque(maxlen=buffer_size)    \n",
    "\n",
    "    #add one transition to the replay buffer\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.replay_memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    #take a random sample from the replay buffer for training\n",
    "    def sample(self, batch_size):\n",
    "        if batch_size <= len(self.replay_memory):\n",
    "            return rand.sample(self.replay_memory, batch_size)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "    #Python magic method to enable len to be used on a replay buffer object\n",
    "    def __len__(self):\n",
    "        return len(self.replay_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0adef140-dbc5-4d23-af0d-ca6704ba62b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T02:54:50.236021Z",
     "iopub.status.busy": "2023-11-20T02:54:50.235229Z",
     "iopub.status.idle": "2023-11-20T02:54:50.243859Z",
     "shell.execute_reply": "2023-11-20T02:54:50.242689Z",
     "shell.execute_reply.started": "2023-11-20T02:54:50.235983Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Class to implement an epsilon decay schedule\n",
    "#epsilon starts high and then reduces by a decay factor\n",
    "#The decay factor can be changed according to how many training iterations\n",
    "#  are completed\n",
    "class EpsilonSchedule():\n",
    "    def __init__(self, final_epsilon=0.1, pre_train_steps=10, final_exploration_step=100):\n",
    "        self.pre_train_steps = pre_train_steps\n",
    "        self.final_exploration_step = final_exploration_step\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.decay_factor = self.pre_train_steps/self.final_exploration_step\n",
    "        self.epsilon = 1\n",
    "    \n",
    "    def value(self, t):\n",
    "        if t > self.pre_train_steps:\n",
    "            self.decay_factor = (t - self.pre_train_steps)/self.final_exploration_step\n",
    "            self.epsilon = 1-self.decay_factor\n",
    "            self.epsilon = max(self.final_epsilon, self.epsilon)\n",
    "            return self.epsilon\n",
    "        else:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb4acae7-3e05-4d56-b800-9cbb217c418a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T02:54:50.245964Z",
     "iopub.status.busy": "2023-11-20T02:54:50.245588Z",
     "iopub.status.idle": "2023-11-20T02:54:50.255819Z",
     "shell.execute_reply": "2023-11-20T02:54:50.254033Z",
     "shell.execute_reply.started": "2023-11-20T02:54:50.245928Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define the neural network model using keras\n",
    "class DQN(keras.Model):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        # self.input_layer = keras.layers.InputLayer(input_shape=input_shape)\n",
    "        # self.hidden_layers = []\n",
    "        \n",
    "                        # tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "        print(input_shape)\n",
    "        self.net = tf.keras.Sequential(\n",
    "            [\n",
    "                # tf.keras.layers.Input(shape=input_shape), #Input shape will be a 210 x 160 rgb image (change 3 to 1 and double check the dimensions of the input image if grayscale is used\n",
    "                keras.layers.InputLayer(input_shape=input_shape),\n",
    "                tf.keras.layers.Conv2D(32, 10, strides = 2, activation = \"relu\", padding = \"same\"), #32 10 x 10 filters with a stride length of 2 and a padding of 0s around the edges of the image\n",
    "                tf.keras.layers.Conv2D(32, 10, activation = \"relu\"), #32 10 x 10 filters with a stride length of 1\n",
    "                tf.keras.layers.Conv2D(64, 10, activation = \"relu\"), #64 10 x 10 filters with a stride length of 1\n",
    "                tf.keras.layers.Conv2D(64, 5, activation = \"relu\"), #64 5 x 5 filters with a stride length of 1\n",
    "                tf.keras.layers.MaxPooling2D(5), #Max Pooling using a 5 x 5 filter\n",
    "                #Max pooling takes the highest value in the filter an makes it the value of a smaller \"image,\" unlike the convolutional layers, the filter does not overlap itself\n",
    "                tf.keras.layers.Flatten(), #Converts the data up to this point into a 1D array\n",
    "                tf.keras.layers.Dense(256, activation = \"relu\"), #Single regression layer for a small boost to feature extraction\n",
    "                tf.keras.layers.Dense(num_actions, activation = \"softmax\") #Determines the class\n",
    "                #Softmax activation will bind the results to a range of [0, 1], with the sum of all nodes equaling 1\n",
    "                #This allows the final layer to present a probability of each action\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # self.hidden_layers.append(keras.layers.Dense(64, activation='relu'))\n",
    "        # self.hidden_layers.append(keras.layers.Dense(32, activation='relu'))\n",
    "        # self.output_layer = keras.layers.Dense(units=num_actions, activation='linear')\n",
    "\n",
    "    #forward pass of the model\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        # z = self.input_layer(inputs)\n",
    "        # for l in self.hidden_layers:\n",
    "        #     z = l(z)\n",
    "        # q_vals = self.output_layer(z)\n",
    "        q_vals = self.net(inputs)\n",
    "        return q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db781b0e-3d8b-4602-9005-c8915431f788",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T02:54:50.258218Z",
     "iopub.status.busy": "2023-11-20T02:54:50.257811Z",
     "iopub.status.idle": "2023-11-20T02:54:50.280306Z",
     "shell.execute_reply": "2023-11-20T02:54:50.278938Z",
     "shell.execute_reply.started": "2023-11-20T02:54:50.258178Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, gamma=0.9, batch_size=64, lr=0.001,\n",
    "                 max_episodes = 500, max_steps_per_episode=2000,\n",
    "                 steps_until_sync=20, choose_action_frequency=1,\n",
    "                 pre_train_steps = 1, final_exploration_step = 100):\n",
    "        \n",
    "        self.env = env\n",
    "        self.input_shape = list(env.observation_space.shape)\n",
    "        self.num_actions = env.action_space.n\n",
    "        # dqn is used to predict Q-values to decide which action to take\n",
    "        self.dqn = DQN(self.input_shape, self.num_actions)\n",
    "        #build is used for subclassed models and takes the input shape as an argument\n",
    "        #build builds the model\n",
    "        self.dqn.build(tf.TensorShape([None, *self.input_shape]))\n",
    "        \n",
    "        # second DQN to predit the future reward of Q(s',a)\n",
    "        # dqn_target is used to predict the future reward\n",
    "        self.dqn_target = DQN(self.input_shape, self.num_actions)\n",
    "        self.dqn_target.build(tf.TensorShape([None, *self.input_shape]))\n",
    "\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        #stochastic gradient method, lr is learning rate\n",
    "        self.optimizer = tf.optimizers.legacy.Adam(lr)\n",
    "        #discount factor\n",
    "        self.gamma = gamma\n",
    "        #to fill up the replay buffer\n",
    "        self.pre_train_steps = pre_train_steps\n",
    "        self.final_exploration_step = final_exploration_step\n",
    "        self.replay_buffer = ReplayBuffer(max_episodes*max_steps_per_episode)\n",
    "        self.epsilon_schedule = EpsilonSchedule(final_epsilon=0.1, \n",
    "                pre_train_steps=self.pre_train_steps,\n",
    "                final_exploration_step=self.final_exploration_step)\n",
    "        #steps until the target dqn is updated with the current dqn\n",
    "        self.steps_until_sync = steps_until_sync\n",
    "        #choose a new action every action frequency steps\n",
    "        self.choose_action_frequency = choose_action_frequency\n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "        #loss function of mean squared error\n",
    "        self.loss_function = tf.keras.losses.MSE\n",
    "        self.episode_reward_history = []\n",
    "\n",
    "    #predict the q values\n",
    "    def predict_q(self, inputs):\n",
    "        return self.dqn(inputs)\n",
    "\n",
    "    def get_action(self, states, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            # explore\n",
    "            return np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            # exploit\n",
    "            return np.argmax(self.predict_q(np.expand_dims(states, axis=0))[0])\n",
    "\n",
    "    #copy dqn into dqn_target\n",
    "    def update_target_network(self):\n",
    "        self.dqn_target.set_weights(self.dqn.get_weights())\n",
    "\n",
    "    #take a training step\n",
    "    def train_step(self):\n",
    "        #take a random sample from the replay buffer\n",
    "        mini_batch = self.replay_buffer.sample(self.batch_size)\n",
    "        #unzip the random sample into separate vectors\n",
    "        observations_batch, action_batch, reward_batch, next_observations_batch, done_batch = map(np.array,zip(*mini_batch))\n",
    "        #record operations for automatic differentiation\n",
    "        with tf.GradientTape() as tape:\n",
    "            #watch the trainable variables\n",
    "            dqn_variables = self.dqn.trainable_variables\n",
    "            tape.watch(dqn_variables)\n",
    "            #compute the rewards of the next state\n",
    "            future_rewards = self.dqn_target(tf.convert_to_tensor(next_observations_batch, dtype=tf.float32))\n",
    "            next_action = tf.argmax(future_rewards, axis=1)\n",
    "            #find the sum of elements across the columns of the tensor\n",
    "            #one_hot is used to mask out any q values that are unneeded\n",
    "            target_q = tf.reduce_sum(tf.one_hot(next_action, self.num_actions) * future_rewards, axis=1)\n",
    "            #update the future rewards eliminating any states that were terminal states\n",
    "            target_q = (1 - done_batch) * self.gamma * target_q + reward_batch\n",
    "            #do the same for the current state\n",
    "            predicted_q = self.dqn(tf.convert_to_tensor(observations_batch, dtype=tf.float32))\n",
    "            predicted_q = tf.reduce_sum(tf.one_hot(action_batch, self.num_actions) * predicted_q, axis=1)\n",
    "            #find the loss between the tartget and the predicted\n",
    "            loss = self.loss_function(target_q, predicted_q)   \n",
    "        # Backpropagate the loss\n",
    "        gradients = tape.gradient(loss, dqn_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, dqn_variables))\n",
    "        #return the loss\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        episode = 0\n",
    "        total_step = 0\n",
    "        episode_step = 0\n",
    "        state, info = self.env.reset()\n",
    "        loss = 0\n",
    "        last_hundred_rewards = deque(maxlen=100)\n",
    "\n",
    "        while episode < self.max_episodes:\n",
    "            current_state, info = self.env.reset()\n",
    "            done = False\n",
    "            action = 0\n",
    "            episode_reward = 0\n",
    "            episode_step = 0\n",
    "            epsilon = self.epsilon_schedule.value(total_step)\n",
    "\n",
    "            while not done:\n",
    "                #control the number of times a new action is chosen\n",
    "                if total_step % self.choose_action_frequency == 0:\n",
    "                    if len(self.replay_buffer) > self.batch_size:\n",
    "                        action = self.get_action(current_state, epsilon)\n",
    "                    else:\n",
    "                        action = self.get_action(current_state, 1.0)\n",
    "\n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                self.replay_buffer.add(current_state, action, reward, next_state, done)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                #train the dqn if enough data samples are available\n",
    "                if total_step > self.pre_train_steps and len(self.replay_buffer) > self.batch_size:\n",
    "                    loss = self.train_step()\n",
    "                #control how often the target dqn is updated in order to foster stability in the target q value\n",
    "                if total_step % self.steps_until_sync == 0:\n",
    "                    self.update_target_network()\n",
    "                                    \n",
    "                #end of step\n",
    "                total_step += 1\n",
    "                episode_step += 1\n",
    "                current_state = next_state\n",
    "                \n",
    "            # end of episode\n",
    "            self.episode_reward_history.append(episode_reward)\n",
    "            last_hundred_rewards.append(episode_reward)\n",
    "            mean_episode_reward = np.mean(last_hundred_rewards)\n",
    "            #show the average reward\n",
    "            if episode % 10 == 0:\n",
    "                print('\\n' + f'Episode {episode} (Step {total_step}) - Moving Avg Reward: {mean_episode_reward:.3f} Loss: {loss:.5f} Epsilon: {epsilon:.3f}')\n",
    "            else:\n",
    "                print(\"*\", end=\"\")\n",
    "            #stop training if the mean of the last 100 rewards is nearing 200\n",
    "            if mean_episode_reward >= 195:\n",
    "                print(f'Task solved after {episode} episodes! (Moving Avg Reward: {mean_episode_reward:.3f})')\n",
    "                return                \n",
    "            episode += 1\n",
    "            \n",
    "    def load_weights(self, pathname):\n",
    "        self.dqn.load_weights(pathname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59c0a951-957f-4bc1-93aa-210e06386cd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T02:54:50.282108Z",
     "iopub.status.busy": "2023-11-20T02:54:50.281745Z",
     "iopub.status.idle": "2023-11-20T02:54:50.515624Z",
     "shell.execute_reply": "2023-11-20T02:54:50.514540Z",
     "shell.execute_reply.started": "2023-11-20T02:54:50.282083Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(18)\n",
      "Action space size: 18\n",
      "Observation space shape: (84, 84, 1)\n",
      "EnvSpec(id='ALE/MarioBros-v5', entry_point='shimmy.atari_env:AtariEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'game': 'mario_bros', 'obs_type': 'rgb', 'repeat_action_probability': 0.25, 'full_action_space': False, 'frameskip': 4, 'max_num_frames_per_episode': 500}, namespace='ALE', name='MarioBros', version=5, additional_wrappers=(WrapperSpec(name='ProcessFrame84', entry_point='__main__:ProcessFrame84', kwargs=None), WrapperSpec(name='ScaledFloatFrame', entry_point='__main__:ScaledFloatFrame', kwargs=None)), vector_entry_point=None)\n"
     ]
    }
   ],
   "source": [
    "GAME = \"ALE/MarioBros-v5\"\n",
    "env = make_env(gym.make(GAME, max_num_frames_per_episode=500))\n",
    "print(\"Action space: {}\".format(env.action_space))\n",
    "print(\"Action space size: {}\".format(env.action_space.n))\n",
    "observation, info = env.reset()\n",
    "print(\"Observation space shape: {}\".format(observation.shape))\n",
    "print(env.spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00abe454-5b02-4020-a2fe-d505ed87ee85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T02:54:50.516872Z",
     "iopub.status.busy": "2023-11-20T02:54:50.516595Z",
     "iopub.status.idle": "2023-11-20T03:17:58.103902Z",
     "shell.execute_reply": "2023-11-20T03:17:58.102721Z",
     "shell.execute_reply.started": "2023-11-20T02:54:50.516846Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 84, 1]\n",
      "[84, 84, 1]\n",
      "\n",
      "Episode 0 (Step 47) - Moving Avg Reward: 0.000 Loss: 0.00000 Epsilon: 1.000\n",
      "*********\n",
      "Episode 10 (Step 517) - Moving Avg Reward: 0.000 Loss: 0.00064 Epsilon: 0.953\n",
      "*********\n",
      "Episode 20 (Step 987) - Moving Avg Reward: 0.000 Loss: 0.00004 Epsilon: 0.906\n",
      "*********\n",
      "Episode 30 (Step 1457) - Moving Avg Reward: 0.000 Loss: 0.00049 Epsilon: 0.859\n",
      "*********\n",
      "Episode 40 (Step 1927) - Moving Avg Reward: 0.000 Loss: 0.00015 Epsilon: 0.812\n",
      "*********\n",
      "Episode 50 (Step 2397) - Moving Avg Reward: 0.000 Loss: 0.00005 Epsilon: 0.765\n",
      "*********\n",
      "Episode 60 (Step 2867) - Moving Avg Reward: 0.000 Loss: 0.00005 Epsilon: 0.718\n",
      "*********\n",
      "Episode 70 (Step 3337) - Moving Avg Reward: 0.000 Loss: 0.00005 Epsilon: 0.671\n",
      "*********\n",
      "Episode 80 (Step 3807) - Moving Avg Reward: 0.000 Loss: 0.00000 Epsilon: 0.624\n",
      "*********\n",
      "Episode 90 (Step 4277) - Moving Avg Reward: 0.000 Loss: 0.00000 Epsilon: 0.577\n",
      "*********\n",
      "Episode 100 (Step 4747) - Moving Avg Reward: 0.000 Loss: 0.00017 Epsilon: 0.530\n",
      "*********\n",
      "Episode 110 (Step 5217) - Moving Avg Reward: 0.000 Loss: 0.00010 Epsilon: 0.483\n",
      "*********\n",
      "Episode 120 (Step 5687) - Moving Avg Reward: 0.000 Loss: 0.00000 Epsilon: 0.436\n",
      "*********\n",
      "Episode 130 (Step 6157) - Moving Avg Reward: 0.000 Loss: 0.00008 Epsilon: 0.389\n",
      "*********\n",
      "Episode 140 (Step 6627) - Moving Avg Reward: 0.000 Loss: 0.00004 Epsilon: 0.342\n",
      "*********\n",
      "Episode 150 (Step 7097) - Moving Avg Reward: 0.000 Loss: 0.00000 Epsilon: 0.295\n",
      "*********\n",
      "Episode 160 (Step 7567) - Moving Avg Reward: 0.000 Loss: 0.00019 Epsilon: 0.248\n",
      "*********\n",
      "Episode 170 (Step 8037) - Moving Avg Reward: 0.000 Loss: 0.00010 Epsilon: 0.201\n",
      "*********\n",
      "Episode 180 (Step 8507) - Moving Avg Reward: 0.000 Loss: 0.00000 Epsilon: 0.154\n",
      "*********\n",
      "Episode 190 (Step 8977) - Moving Avg Reward: 0.000 Loss: 0.00005 Epsilon: 0.107\n",
      "*********\n",
      "Episode 200 (Step 9447) - Moving Avg Reward: 0.000 Loss: 0.00005 Epsilon: 0.100\n",
      "*********\n",
      "Episode 210 (Step 9917) - Moving Avg Reward: 0.000 Loss: 0.00000 Epsilon: 0.100\n",
      "*********\n",
      "Episode 220 (Step 10387) - Moving Avg Reward: 0.000 Loss: 0.00019 Epsilon: 0.100\n",
      "*********\n",
      "Episode 230 (Step 10857) - Moving Avg Reward: 0.000 Loss: 0.00010 Epsilon: 0.100\n",
      "*********\n",
      "Episode 240 (Step 11327) - Moving Avg Reward: 0.000 Loss: 0.00005 Epsilon: 0.100\n",
      "*********\n",
      "Episode 250 (Step 11797) - Moving Avg Reward: 0.000 Loss: 0.00005 Epsilon: 0.100\n",
      "*********\n",
      "Episode 260 (Step 12267) - Moving Avg Reward: 0.000 Loss: 0.00005 Epsilon: 0.100\n",
      "*********\n",
      "Episode 270 (Step 12737) - Moving Avg Reward: 0.000 Loss: 0.00009 Epsilon: 0.100\n",
      "*********\n",
      "Episode 280 (Step 13207) - Moving Avg Reward: 0.000 Loss: 0.00001 Epsilon: 0.100\n",
      "*********\n",
      "Episode 290 (Step 13677) - Moving Avg Reward: 0.000 Loss: 0.00005 Epsilon: 0.100\n",
      "*********\n",
      "Episode 300 (Step 14147) - Moving Avg Reward: 0.000 Loss: 0.00011 Epsilon: 0.100\n",
      "*********\n",
      "Episode 310 (Step 14617) - Moving Avg Reward: 0.000 Loss: 0.00005 Epsilon: 0.100\n",
      "*********\n",
      "Episode 320 (Step 15087) - Moving Avg Reward: 0.000 Loss: 0.00005 Epsilon: 0.100\n",
      "*********\n",
      "Episode 330 (Step 15557) - Moving Avg Reward: 0.000 Loss: 0.00004 Epsilon: 0.100\n",
      "*********\n",
      "Episode 340 (Step 16027) - Moving Avg Reward: 0.000 Loss: 0.00011 Epsilon: 0.100\n",
      "*********\n",
      "Episode 350 (Step 16497) - Moving Avg Reward: 0.000 Loss: 0.00000 Epsilon: 0.100\n",
      "*********\n",
      "Episode 360 (Step 16967) - Moving Avg Reward: 0.000 Loss: 0.00003 Epsilon: 0.100\n",
      "*********\n",
      "Episode 370 (Step 17437) - Moving Avg Reward: 0.000 Loss: 0.00000 Epsilon: 0.100\n",
      "*********\n",
      "Episode 380 (Step 17907) - Moving Avg Reward: 0.000 Loss: 0.00000 Epsilon: 0.100\n",
      "*********\n",
      "Episode 390 (Step 18377) - Moving Avg Reward: 0.000 Loss: 0.00002 Epsilon: 0.100\n",
      "*********\n",
      "Episode 400 (Step 18847) - Moving Avg Reward: 0.000 Loss: 0.00010 Epsilon: 0.100\n",
      "*********\n",
      "Episode 410 (Step 19317) - Moving Avg Reward: 0.000 Loss: 0.00007 Epsilon: 0.100\n",
      "*********\n",
      "Episode 420 (Step 19787) - Moving Avg Reward: 0.000 Loss: 0.00000 Epsilon: 0.100\n",
      "*********\n",
      "Episode 430 (Step 20257) - Moving Avg Reward: 0.000 Loss: 0.00013 Epsilon: 0.100\n",
      "*********\n",
      "Episode 440 (Step 20727) - Moving Avg Reward: 0.000 Loss: 0.00000 Epsilon: 0.100\n",
      "*********\n",
      "Episode 450 (Step 21197) - Moving Avg Reward: 0.000 Loss: 0.00031 Epsilon: 0.100\n",
      "*********\n",
      "Episode 460 (Step 21667) - Moving Avg Reward: 0.000 Loss: 0.00015 Epsilon: 0.100\n",
      "*********\n",
      "Episode 470 (Step 22137) - Moving Avg Reward: 0.000 Loss: 0.00000 Epsilon: 0.100\n",
      "*********\n",
      "Episode 480 (Step 22607) - Moving Avg Reward: 0.000 Loss: 0.00013 Epsilon: 0.100\n",
      "*********\n",
      "Episode 490 (Step 23077) - Moving Avg Reward: 0.000 Loss: 0.00003 Epsilon: 0.100\n",
      "*********"
     ]
    }
   ],
   "source": [
    "#Train the agent\n",
    "env = make_env(gym.make(GAME, mode=4, max_num_frames_per_episode=200))\n",
    "\n",
    "agent = Agent(env, gamma=0.99, batch_size=64, lr=0.0007, max_episodes=500,\n",
    "              steps_until_sync=500, choose_action_frequency=1,\n",
    "              pre_train_steps = 1, final_exploration_step = 10_000)\n",
    "agent.train()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5da0a65-fc50-4eeb-83ef-0eff088ba5c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-20T04:11:11.061367Z",
     "iopub.status.busy": "2023-11-20T04:11:11.060992Z",
     "iopub.status.idle": "2023-11-20T04:11:37.054532Z",
     "shell.execute_reply": "2023-11-20T04:11:37.053185Z",
     "shell.execute_reply.started": "2023-11-20T04:11:11.061343Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward is: 0.0\n",
      "Total reward is: 0.0\n",
      "Total reward is: 0.0\n",
      "Total reward is: 0.0\n",
      "Total reward is: 0.0\n",
      "Total reward is: 0.0\n",
      "Total reward is: 0.0\n",
      "Total reward is: 0.0\n",
      "Total reward is: 0.0\n",
      "Total reward is: 0.0\n"
     ]
    }
   ],
   "source": [
    "#use the DQN\n",
    "env = make_env(gym.make(GAME, render_mode=\"rgb_array\"))\n",
    "observation, info = env.reset()\n",
    "\n",
    "# Create a VideoWriter object.\n",
    "video_writer = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'MP4V'), 25, (210, 160))\n",
    "\n",
    "\n",
    "#show the steps the agent takes using the optimal policy table\n",
    "for i in range(10):\n",
    "    observation, info = env.reset()\n",
    "    terminated = truncated = False\n",
    "    rewards = 0\n",
    "    while not terminated and not truncated:\n",
    "        #find max policy\n",
    "        Q_values = agent.predict_q(np.expand_dims(observation, axis=0))\n",
    "        action = np.argmax(Q_values[0])\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        video_writer.write(np.uint8(env.render()))\n",
    "        rewards += reward\n",
    "    print('Total reward is: '+str(rewards))\n",
    "env.close()\n",
    "\n",
    "# Close the VideoWriter object.\n",
    "video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4798893-1c67-42c6-924e-02e72062bc89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addfd9c1-483d-4f85-b18f-c35dfb6ec45d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0203d703-54a8-4ec9-a4d1-460e62a3ffd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
