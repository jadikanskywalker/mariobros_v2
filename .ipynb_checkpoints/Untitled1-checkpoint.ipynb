{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62df1195-d33b-43d9-a9e5-4254c59e3227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import cv2\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf9db43e-4584-4191-969d-3f3a44bcf9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import agent_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "632dd480-e466-4fd8-83df-adbf2d550daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(18)\n",
      "Action space size: 18\n",
      "Observation space shape: (84, 84, 1)\n",
      "Environment spec:  EnvSpec(id='ALE/MarioBros-v5', entry_point='shimmy.atari_env:AtariEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'game': 'mario_bros', 'obs_type': 'rgb', 'repeat_action_probability': 0.25, 'full_action_space': False, 'frameskip': 4, 'max_num_frames_per_episode': 108000, 'mode': 4}, namespace='ALE', name='MarioBros', version=5, additional_wrappers=(WrapperSpec(name='ProcessFrame84', entry_point='agent_model:ProcessFrame84', kwargs=None), WrapperSpec(name='ScaledFloatFrame', entry_point='agent_model:ScaledFloatFrame', kwargs=None)), vector_entry_point=None)\n"
     ]
    }
   ],
   "source": [
    "GAME = \"ALE/MarioBros-v5\"\n",
    "\n",
    "#Make environment\n",
    "env = agent_model.make_env(gym.make(GAME, mode=4))\n",
    "print(\"Action space: {}\".format(env.action_space))\n",
    "print(\"Action space size: {}\".format(env.action_space.n))\n",
    "observation, info = env.reset()\n",
    "print(\"Observation space shape: {}\".format(observation.shape))\n",
    "print(\"Environment spec: \", env.spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24edc58f-1454-44d0-97c0-8c8b23ccce81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 84, 1]\n",
      "[84, 84, 1]\n"
     ]
    }
   ],
   "source": [
    "# Traing the agent\n",
    "agent = agent_model.Agent(env, gamma=0.999, batch_size=64, lr=0.0007, max_episodes=1000,\n",
    "              max_steps_per_episode=2000,\n",
    "              steps_until_sync=20, choose_action_frequency=1,\n",
    "              pre_train_steps = 10, final_exploration_step = 700_000)\n",
    "agent.load_weights(\"\")\n",
    "# agent.train()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce6407b-b1be-4773-9899-77a96b7276b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_reshape_dispatcher() got an unexpected keyword argument 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlives\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m num_lives: \u001b[38;5;66;03m# penalize agent when life lost\u001b[39;00m\n\u001b[1;32m     25\u001b[0m         reward \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.33\u001b[39m\n\u001b[0;32m---> 27\u001b[0m     video_writer\u001b[38;5;241m.\u001b[39mwrite(cv2\u001b[38;5;241m.\u001b[39mcvtColor(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m210\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m160\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muint8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2BGR))\n\u001b[1;32m     28\u001b[0m     rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal reward is: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(rewards))\n",
      "File \u001b[0;32m<__array_function__ internals>:179\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: _reshape_dispatcher() got an unexpected keyword argument 'dtype'"
     ]
    }
   ],
   "source": [
    "# Test the agent\n",
    "env = agent_model.make_env(gym.make(GAME, mode=4, render_mode=\"rgb_array\"))\n",
    "observation, info = env.reset()\n",
    "\n",
    "# create a VideoWriter object.\n",
    "video_fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "video_writer = cv2.VideoWriter('./output_videos/test_output_' + str(datetime.datetime.now()) + '.avi', -1, 20.0, (160, 210), isColor=True)\n",
    "\n",
    "#show the steps the agent takes using the optimal policy table\n",
    "for i in range(2):\n",
    "    observation, info = env.reset()\n",
    "    terminated = truncated = False\n",
    "    rewards = 0\n",
    "    while not terminated and not truncated:\n",
    "        #find max policy\n",
    "        Q_values = agent.predict_q(np.expand_dims(observation, axis=0))\n",
    "        action = np.argmax(Q_values[0])\n",
    "        \n",
    "        num_lives = info[\"lives\"]\n",
    "            \n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        reward /= 800\n",
    "        if info[\"lives\"] < num_lives: # penalize agent when life lost\n",
    "            reward -= 0.33\n",
    "                    \n",
    "        video_writer.write(cv2.cvtColor(np.uint8(np.reshape(env.render(), (210, 160, 3))), cv2.COLOR_RGB2BGR))\n",
    "        rewards += reward\n",
    "    print('Total reward is: '+str(rewards))\n",
    "env.close()\n",
    "\n",
    "# Close the VideoWriter object.\n",
    "video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ae564e-7d08-4e0a-80b8-18031f1e80ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
