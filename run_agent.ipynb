{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62df1195-d33b-43d9-a9e5-4254c59e3227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import cv2\n",
    "import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf9db43e-4584-4191-969d-3f3a44bcf9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import agent_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "632dd480-e466-4fd8-83df-adbf2d550daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(18)\n",
      "Action space size: 18\n",
      "Observation space shape: (84, 84, 1)\n",
      "Environment spec:  EnvSpec(id='ALE/MarioBros-v5', entry_point='shimmy.atari_env:AtariEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'game': 'mario_bros', 'obs_type': 'rgb', 'repeat_action_probability': 0.25, 'full_action_space': False, 'frameskip': 4, 'max_num_frames_per_episode': 108000, 'mode': 4}, namespace='ALE', name='MarioBros', version=5, additional_wrappers=(WrapperSpec(name='ProcessFrame84', entry_point='agent_model:ProcessFrame84', kwargs=None), WrapperSpec(name='ScaledFloatFrame', entry_point='agent_model:ScaledFloatFrame', kwargs=None)), vector_entry_point=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "GAME = \"ALE/MarioBros-v5\"\n",
    "\n",
    "#Make environment\n",
    "env = agent_model.make_env(gym.make(GAME, mode=4))\n",
    "print(\"Action space: {}\".format(env.action_space))\n",
    "print(\"Action space size: {}\".format(env.action_space.n))\n",
    "observation, info = env.reset()\n",
    "print(\"Observation space shape: {}\".format(observation.shape))\n",
    "print(\"Environment spec: \", env.spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24edc58f-1454-44d0-97c0-8c8b23ccce81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 84, 1]\n",
      "[84, 84, 1]\n",
      "run:  7\n",
      "*********\n",
      "Episode 250 (Step 85714) - Moving Avg Reward: -0.6900 Loss: 0.01796 Epsilon: 0.100 Avg Steps Per Episode: 769.1\n",
      "*********\n",
      "Episode 260 (Step 92767) - Moving Avg Reward: -0.9900 Loss: 0.00000 Epsilon: 0.100 Avg Steps Per Episode: 705.3\n",
      "*********\n",
      "Episode 270 (Step 99315) - Moving Avg Reward: -0.9900 Loss: 0.00000 Epsilon: 0.100 Avg Steps Per Episode: 654.8\n",
      "*********\n",
      "Episode 280 (Step 105796) - Moving Avg Reward: -0.3900 Loss: 0.01565 Epsilon: 0.100 Avg Steps Per Episode: 648.1\n",
      "*********\n",
      "Episode 290 (Step 111563) - Moving Avg Reward: -0.4900 Loss: 0.00000 Epsilon: 0.100 Avg Steps Per Episode: 576.7\n",
      "*********\n",
      "Episode 300 (Step 117979) - Moving Avg Reward: -0.5900 Loss: 0.00403 Epsilon: 0.100 Avg Steps Per Episode: 641.6\n",
      "*********\n",
      "Episode 310 (Step 125330) - Moving Avg Reward: -0.7900 Loss: 0.00000 Epsilon: 0.100 Avg Steps Per Episode: 735.1\n",
      "*********\n",
      "Episode 320 (Step 131597) - Moving Avg Reward: -0.9900 Loss: 0.00000 Epsilon: 0.100 Avg Steps Per Episode: 626.7\n",
      "*********\n",
      "Episode 330 (Step 138232) - Moving Avg Reward: -0.8900 Loss: 0.00000 Epsilon: 0.100 Avg Steps Per Episode: 663.5\n",
      "*********\n",
      "Episode 340 (Step 148428) - Moving Avg Reward: -0.8900 Loss: 0.00001 Epsilon: 0.100 Avg Steps Per Episode: 1019.6\n",
      "****"
     ]
    }
   ],
   "source": [
    "# Traing the agent\n",
    "agent = agent_model.Agent(env, gamma=0.99, batch_size=64, lr=0.002, max_episodes=1000,\n",
    "              max_steps_per_episode=2000,\n",
    "              steps_until_sync=20, choose_action_frequency=1,\n",
    "              pre_train_steps = 1, final_exploration_step = 50_000)\n",
    "\n",
    "# You can configure the agent to resume training from a past run:\n",
    "\n",
    "#   1. load past weights to the model:\n",
    "agent.load_weights(\"./checkpoints_v7/ep_240\")\n",
    "\n",
    "#   2. load past replay buffers (so that the model can train on past moves):\n",
    "with open(\"./output_replay_memory/run7_ep240\", 'rb') as f:\n",
    "    agent.replay_buffer.replay_memory = pickle.load(f)\n",
    "\n",
    "#   3. set `start_ep` and `start_step` to where the last run left off (affects printing the the epsilon schedule)\n",
    "agent.train(start_ep=241, start_step=78023, eps_until_save=10, save_checkpoints=True)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce6407b-b1be-4773-9899-77a96b7276b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent\n",
    "env = agent_model.make_env(gym.make(GAME, mode=4, render_mode=\"rgb_array\"))\n",
    "observation, info = env.reset()\n",
    "\n",
    "# create a VideoWriter object.\n",
    "video_fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "video_writer = cv2.VideoWriter('./output_videos/test_output_' + str(datetime.datetime.now()) + '.avi', -1, 20.0, (160, 210), isColor=True)\n",
    "\n",
    "#show the steps the agent takes using the optimal policy table\n",
    "for i in range(2):\n",
    "    observation, info = env.reset()\n",
    "    terminated = truncated = False\n",
    "    rewards = 0\n",
    "    while not terminated and not truncated:\n",
    "        #find max policy\n",
    "        Q_values = agent.predict_q(np.expand_dims(observation, axis=0))\n",
    "        action = np.argmax(Q_values[0])\n",
    "        \n",
    "        num_lives = info[\"lives\"]\n",
    "            \n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        reward /= 800\n",
    "        if info[\"lives\"] < num_lives: # penalize agent when life lost\n",
    "            reward -= 0.33\n",
    "                    \n",
    "        video_writer.write(cv2.cvtColor(np.uint8(np.reshape(env.render(), (210, 160, 3))), cv2.COLOR_RGB2BGR))\n",
    "        rewards += reward\n",
    "    print('Total reward is: '+str(rewards))\n",
    "env.close()\n",
    "\n",
    "# Close the VideoWriter object.\n",
    "video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39ae564e-7d08-4e0a-80b8-18031f1e80ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./output_replay_memory/run7_ep240\", 'wb') as f:\n",
    "    pickle.dump(agent.replay_buffer.replay_memory, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ffebb4-64d0-4610-8d4f-7af1a76fa8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./output_replay_memory/run7_ep240\", 'r' as f:\n",
    "    agent.replay_buffer.replay_memory = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
